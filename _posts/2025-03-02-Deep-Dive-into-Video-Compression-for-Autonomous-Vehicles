# Diving Deep into Video Token Compression for Autonomous Vehicles

I've spent the last few weeks obsessing over a fascinating challenge: squeezing more compression out of already-compressed video tokens used for training autonomous driving systems. Like most rabbit holes I find myself in, this one started innocently enough with a simple question - "Can we further compress these VQ-VAE tokens?" - and quickly spiraled into a full-blown exploration of compression algorithms, bit manipulation, and the fundamental limits of information theory.

The tokens I've been working with come from Comma AI's driving system, where front-facing camera footage gets processed through a Vector Quantized Variational Autoencoder (VQ-VAE). This encoder transforms raw video into a compact representation - essentially a codebook of tokens that capture the essential visual information needed for their world model to make driving decisions. My challenge was to see if I could compress these tokens even further without losing critical information.

What makes this particularly interesting is that we're attempting to compress data that's already been compressed. It's like trying to squeeze a sponge that's already been wrung out - finding those last few drops requires increasingly clever techniques. The target was an ambitious 3.0x compression ratio, which would dramatically reduce storage requirements and potentially speed up training for these driving models.

I'll admit I was a bit stubborn about this project - the official compression challenge ended months ago, but I couldn't let it go. There's something irresistible about optimization problems, especially ones where you're pushing against the theoretical limits of what's possible. Plus, working with solveit made it easy to rapidly test different approaches that would have taken me ages to code from scratch.

So grab a coffee and join me as I walk through this journey of compression algorithms, vectorization tricks, and the inevitable trade-offs between perfect accuracy and practical efficiency. Even if you're not building autonomous vehicles, the techniques we'll explore have applications anywhere you need to efficiently store or transmit large amounts of structured data.

## Understanding the Data: VQ-VAE Tokens and Driving Videos

Before diving into compression techniques, I needed to wrap my head around what I was actually working with. These aren't your standard image files or video frames - they're tokens generated by a Vector Quantized Variational Autoencoder (VQ-VAE), which is a fancy way of saying "a neural network that compresses images into discrete codes."

Each segment of data represents about a minute of driving footage captured at 20 frames per second, resulting in 1200 frames per segment. But instead of raw pixels, each frame is encoded as a grid of 8×16 tokens - 128 tokens total per frame. Each token is a 10-bit integer that essentially points to an entry in a learned codebook. Think of it as the neural network saying, "This patch of road looks like pattern #739" instead of storing all the pixel values.

When I looked at the original encoder code, I found that the pipeline starts with H.265 video from a front-facing camera mounted under a car's rearview mirror. This video gets fed through the VQ-VAE encoder, which outputs these compact token representations.

```
frames = read_video("../examples/sample_video_ecamera.hevc")
frames = np.array([transform_img(x) for x in frames])
frames = torch.from_numpy(frames).permute(0,3,1,2).to(device='cuda').float()

# load model
config = CompressorConfig()
with torch.device('meta'):
  encoder = Encoder(config)
encoder.load_state_dict_from_url('https://huggingface.co/commaai/commavq-gpt2m/resolve/main/encoder_pytorch_model.bin', assign=True)
encoder = encoder.eval().to(device='cuda')
```

What makes this compression challenge particularly tricky is that we're trying to compress data that's already been compressed once. The VQ-VAE has already done the heavy lifting of reducing high-dimensional video frames down to a sparse representation. It's designed to throw away redundant information while keeping the essential visual features needed for driving.

The data structure itself is quite clean - a numpy array of shape (1200, 8, 16) with int16 values. But despite its simple format, these tokens are information-dense. They're not like natural images where large regions might have the same color, or text where certain patterns repeat frequently. Each token carries significant information about the visual scene, making traditional compression techniques less effective.

As I experimented with visualizing the token patterns, I noticed some interesting properties:
- Temporal coherence: Consecutive frames often have similar token patterns, especially during steady driving
- Spatial structure: Certain regions of the 8×16 grid (like the road area) show more consistent patterns than others
- Token distribution: The distribution of token values isn't uniform - some codebook entries appear much more frequently than others

These observations gave me some initial ideas about where compression gains might be found. But I quickly realized that achieving our ambitious 3.0x compression target would require getting creative - we'd need to exploit every pattern and redundancy in the data, no matter how subtle.

## Leveraging and tweaking Comma's Compression Pipeline: From Concept to Code

With a better understanding of the data, I needed to set up a proper compression pipeline to experiment with different approaches. Like any good tinkerer, I wanted a system that would let me rapidly test ideas, measure results, and iterate quickly.

My first step was to create a basic framework with compress.py and decompress.py files. The compress.py would handle taking the VQ-VAE tokens and squeezing them down, while decompress.py would do the reverse operation, hopefully giving back exactly what we put in (this lossless requirement would later become an interesting discussion point).

```python
def compress_tokens(tokens: np.ndarray) -> bytes:
    """This is where the magic happens (or at least where I try to make it happen)"""
    # Transform and compress the tokens somehow
    return compressed_bytes

def decompress_bytes(x: bytes) -> np.ndarray:
    """This needs to perfectly reverse whatever compress_tokens does"""
    # Decompress and restore the original tokens
    return original_tokens
```

One of the first things I realized was that running compression tests on the full dataset would be painfully slow. When you're iterating on ideas, waiting minutes (or hours!) for results is a creativity killer. So I added a development mode that would only process a small subset of the data:

```python
# DEVELOPMENT MODE: Select small subset for faster testing
dev_mode = True  # Set to False for full dataset processing
if dev_mode:
    # Select just 25 examples for quick development cycles
    for split in ds:
        ds[split] = ds[split].select(range(min(25, len(ds[split]))))
    print(f"DEVELOPMENT MODE: Testing on {sum(len(ds[s]) for s in ds)} examples")
```

This simple change reduced my test cycles from minutes to seconds - a game-changer for rapid experimentation. I also added detailed reporting to see not just the overall compression ratio, but stats for individual files:

```python
# Print summary statistics
print(f"\nCompression Statistics:")
print(f"  Min: {min(all_rates):.2f}x")
print(f"  Max: {max(all_rates):.2f}x")
print(f"  Avg: {sum(all_rates)/len(all_rates):.2f}x")
print(f"  Med: {sorted(all_rates)[len(all_rates)//2]:.2f}x")
```

Another critical component was error detection. When you're manipulating bytes and bits, it's frighteningly easy to make a small mistake that corrupts your data. I added detailed error reporting that would show exactly where decompression failed:

```python
if not np.all(decompressed == original):
    # Find where the differences are
    diff_mask = (decompressed != original)
    num_diffs = np.sum(diff_mask)
    diff_indices = np.where(diff_mask)
    
    # Get some example differences
    sample_diffs = []
    for i in range(min(5, len(diff_indices[0]))):
        idx = tuple(dim[i] for dim in diff_indices)
        sample_diffs.append((idx, original[idx], decompressed[idx]))
    
    error_msg = f"Decompression error for {path}:\n"
    error_msg += f"  - {num_diffs} differences found\n"
    error_msg += f"  - Sample differences (idx, original, decompressed):\n"
    for diff in sample_diffs:
        error_msg += f"    {diff}\n"
```

This detailed error reporting saved me countless hours - instead of just knowing something was wrong, I could see exactly which tokens were affected and how they were being changed.

I also created a comprehensive testing framework that could evaluate different compression methods side by side. This was invaluable for comparing approaches and understanding their trade-offs:

```python
def test_compression_methods(file_path):
    """Test different compression methods on a single file"""
    # Define compression methods to test
    compression_methods = {
        "Minimal (Direct LZMA)": {...},
        "Simple Transpose": {...},
        "Delta Encoding": {...},
        "Original Custom": {...}
    }
    
    # Test each method and report results
    for method_name, funcs in compression_methods.items():
        print(f"\n==== Testing {method_name} ====")
        # ...test and report results...
```

With this pipeline in place, I had a solid foundation for experimentation. The fast feedback loop meant I could try wild ideas without committing hours to each one. And the detailed error reporting meant I could quickly understand what was working and what wasn't.

This setup proved essential because, as I was about to discover, compressing already-compressed tokens would require trying a lot of different approaches before finding something that worked well.

